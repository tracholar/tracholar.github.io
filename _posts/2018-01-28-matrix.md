---
layout: post
title: "深入理解矩阵"
description: ""
category: "math"
tags: ["矩阵","线性代数"]
---

刚开始学习矩阵的时候，总会很好奇，为什么会有矩阵，矩阵的乘法运算为什么是这个样子，矩阵的意义到底是什么？
这些疑惑，随着学习的深入，会逐渐被揭开，本文就是作者的疑惑被揭开后的总结，希望对刚学习线性代数的读者有所帮助，对想深入理解矩阵的读者也是值得一看的文章。

* 目录
{:toc}

## 矩阵的本质

### 矩阵的线性变换视角
相信很多读者第一次了解矩阵都是从解线性方程开始的，我们将从一个很接近的角度来得到矩阵，认为矩阵的本质就是 **有限维线性空间中的线性变换**.

我们还是从一维的情况开始吧，也就是标量的线性变换 $$y = ax$$。所谓一个变换$$y = f(x)$$是线性的，是指这个变换遵循两个线性运算的规律：

1. 对于数乘可以交换，$$f(\lambda x) = \lambda f(x)$$
2. 对于元素加法可交换， $$f(x_1 + x_2) = f(x_1) + f(x_2)$$

对于第一条规律，标量线性变换显然是满足的，因为$$a \cdot(\lambda x) = \lambda (a x)$$，这是由乘法结合律带来的。
对于第二条规律，利用乘法对加法的分配率也容易验证，$$a(x_1+x_2) = a x_1 + a x_2$$。
简单地理解，**线性变换就是没有常数项的一次函数**！
显然，一维情况的线性变换可以只用一个常数a来描述就可以了，不需要其他参数！

那么，对于高维情况呢？我们考虑最简单的二维情况，一般情况下，二维的线性变换可以表达为没有常数项的二元一次函数

$$
y_1 = a_{11} x_1 + a_{12} x_2 \\
y_2 = a_{21} x_1 + a_{22} x_2
$$

相比一维情况只要1个参数就可以描述，二维则需要4个参数！依次类推下去，n维线性变换自然需要$$n^2$$个参数才能描述！
把这些参数排列成方正就是我们的矩阵了！

$$
A = \left[ \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix} \right]
$$

因此，每一个矩阵都是有限维线性空间中的一个线性变换。线性方程组只不过是已知线性变换和像求原像的问题。
反过来，是否有限维线性空间中的任意线性变换都一定可以用一个矩阵表示出来呢？
答案是肯定的！这就是说，**矩阵和有限维线性空间中的线性变换是一一对应**！
考虑线性变换$$y = f(x), x\in R^n, y\in R^m$$，
令$$\{e_i,i=1,..n\}$$是$$R^n$$中的一组标准正交基，$$\{g_i,i=1,..m\}$$是$$R^m$$中的一组标准正交基，线性代数的知识告诉我们一定可以找到这样两组组标准正交基！
那么，利用线性变量的两组性质可得

$$
f(x) = f(\sum_i x_i e_i) = \sum_i x_i f(e_i)
$$

这里$$x_i$$是向量$$x$$在标准正交基下的坐标！又因为$$f(e_i) \in R^m$$，根据线性代数的知识可知，一定可以把它表示成标准正交基的线性组合

$$
f(e_i) = \sum_j a_{ji} g_j, i=1,...,n
$$

代入上式可得

$$
y = f(x) =  \sum_j \left( \sum_i a_{ji} x_i \right) g_j
$$

因为$$g_j$$是标准正交基，所以前面的数就是y在这组基下的坐标

$$
y_i = \sum_j a_{ij} x_j
$$

这表明线性变换$$f$$可以用矩阵$$A=[a_{ij}]$$表示出来！

对于有限维线性空间，矩阵和线性变换一一对应，那么对于无限维线性空间呢？
泛函分析就是研究无限维线性空间的理论，在无限维线性空间中，线性算子有很多和矩阵相似的性质，但是一般很难用矩阵表示出来。

### 矩阵的坐标框架视角
矩阵另外一个图像是坐标框架转换，前面我们把坐标框架——也就是基向量固定，让向量进行变换，我们也可以反过来，让向量不变，重新选取一组坐标基。运动是相对的，所以这两种图像实际上是等价的！
假设在坐标框架$$\{e_i, i=1,..,n\}$$下，向量$$v = \sum_i x_i e_i$$，$$x_i$$是在这组基下的坐标。
现在我们重新选取一组基$$\{g_i, i=1,...,n\}$$，向量$$v = \sum_i y_i g_i$$，$$y_i$$是在这组基下的坐标。
那么这两组坐标之间有什么关系呢？
因为$$g_i$$也是向量，所以它应该也可以用$$e_i$$表示，不妨设$$g_j = \sum_i a_{ij} e_i$$，那么

$$
v = \sum_j y_j g_j \\
 = \sum_i (\sum_j y_j a_{ij}) e_i
$$

利用向量空间唯一表示定理，即向量v在$$e_i$$框架下的坐标是唯一的！所以有 $$x_i = \sum_j a_{ij} y_j$$！
可以看到，坐标的变换系数矩阵实际上就是坐标框架基向量的变换矩阵。
这种图像，在描述某些场景时更方便，例如傅里叶变换就可以看做同一个函数在时域基$$\{\delta(t - s); s \in R\}$$表示到频域基$$\{e^{-i wt}; w\in R\}$$表示的变换，详情可以参考[深入理解傅里叶变换](https://tracholar.github.io/math/2017/03/12/fourier-transform.html)这篇文章！

### 矩阵的张量积视角

如果我们把矩阵也看做线性空间中的向量，那么它也应该有基向量，或者叫做基矩阵更合适。
[张量积](https://en.wikipedia.org/wiki/Tensor_product)可以用来从低阶张量构造高阶张量，向量是一阶张量，矩阵是二阶张量，可以利用张量积从向量构造出矩阵！

$$
A = \sum_i \sum_j a_{ij} g_i \otimes e_j
$$

这里假设都是正交基，$$g_i \otimes e_j$$是两个基向量的张量积，也可以看做矩阵的基矩阵！矩阵对向量的变换可以看做内积运算

$$
Ax = \sum_i \sum_j a_{ij} x_j (g_i \otimes e_j, e_j) \\
=\sum_i a_{ij} x_j g_i
$$

这表明矩阵对向量x的内积相当于完成了向量x从一组基到另外一组基的重新表示！

量子力学中，常用狄拉克括号表示向量及运算，张量积视角就更加自然了

$$
A = \sum_i \sum_j a_{ij} |i\rangle \langle j| \\
x = \sum_i x_i |i\rangle \\
Ax = \sum_i\sum_j\sum_k a_{ij} x_k |i\rangle \langle j| k \rangle =\sum_i\sum_j a_{ij} x_j |i\rangle
$$

## 矩阵的乘法

我在第一次学习矩阵的乘法的时候，十分好奇为什么矩阵乘法是那样的定义。
这里，我们通过线性变换的角度比较容易说明。

我们还是从一维的情况说起吧，现在我有两个线性变换 $$y = a x, z = b y$$，我想用一个变换$$z = c x$$来代替，
那么，显然有$$c= b\cdot a$$！也就是说，在一维情况，两个数的乘法实际上可以看做是用一个线性变换$$c$$来代替两个变换$$a, b$$的依次变换的结果，即

$$
z = c \cdot x = b \cdot (a \cdot x)
$$

我们把这个思想运用到高维不就可以得到矩阵乘法了吗？
假设两个矩阵为A和B，对应的变换是$$y = A x, z = B y$$，我们想用一个矩阵C来代替这两个变换的依次作用的效果，即

$$
z = C x = B(A x)
$$

我们把上式右边展开可得

$$
\begin{align*}
z_{11} =& b_{11}(a_{11}x_1 + ... a_{1n} x_n) + ... + b_{1n}(a_{n1}x_1 + ... a_{nn} x_n) \\
= &\left(b_{11} a_{11} + ... + b_{1n} a_{n1}\right) x_1 + \\
  &\left(b_{11} a_{12} + ... + b_{1n} a_{n2}\right) x_2 + \\
  & ...\\
  &\left(b_{11} a_{1n} + ... + b_{1n} a_{nn}\right) x_n
\end{align*}
$$

因此可知

$$
c_{11} = b_{11} a_{11} + ... + b_{1n} a_{n1} \\
...\\
c_{1n} = b_{11} a_{1n} + ... + b_{1n} a_{nn}
$$

把上述代入过程用求和符号改写可以简化为

$$
z_{i} = \sum_j c_{ij} x_j \\
= \sum_k b_{ik} \left(\sum_j a_{kj} x_j\right) \\
= \sum_j \left(\sum_k b_{ik} a_{kj} \right) x_j
$$

最后一个等式利用了求和符号的交换，这表明

$$
c_{ij} = \sum_k b_{ik} a_{kj}
$$

这就是矩阵乘法的运算法则，把B矩阵第i行与A矩阵第j列的内积作为C矩阵的第(i, j)个元素的值！
这表明，矩阵乘法运算法则是一种必然的结果，而不是某种人为规定的奇怪的运算法则！
矩阵的乘法，实际上就是两个线性变换的乘法，变换的乘法运算就是定义为依次映射，

$$
(B \cdot A) x = B \cdot (A x)
$$

**这种定义是为了让乘法满足结合律很自然的结果**！

## 矩阵的特征值和特征向量
在学习特征值和特征向量的时候，很多人都会很迷惑，这搞得是啥玩意儿！？但是，只要把来龙去脉搞清楚，就不难理解了。
特征值和特征向量是最重要的概念之一！

我们还是从一维开始说起，一维的线性变换就是简单的把x放大a倍！
但是，这个直观的图像在高维情况就不那么直观了！我们还是从二维说起。
这种不直观的根本原因在于交叉项$$a_{12}, a_{21}$$的存在，如果这两项为0，那么矩阵就是对角阵

$$
A = \left[ \begin{matrix} a_{11} & 0 \\ 0 & a_{22} \end{matrix} \right]
$$

这个对角矩阵有个直观的图像，经过它变换的向量，在$$x, y$$两个方向上分别放大$$a_{11}, a_{22}$$倍！
这种情况，我们也说在这种变换下，不同坐标间没有耦合，各坐标是独立伸缩变换的！

对于一般的方阵，是否也有类似的图像？显然对于一般的方阵A，肯定不是在$$x, y$$两个方向上的独立伸缩变换。
那是不是可以找到两个独立的方向呢？如果能够找到，那么方阵A可以看做一个旋转变换加上两个独立方向上的伸缩变换，最后再旋转回来这三个变换构成！写成公式就是

$$
A = T^{-1} \Lambda T
$$

其中$$T$$是一个旋转变换，$$\Lambda$$是对角方阵代表两个方向上的独立伸缩变换。
没错，这就是矩阵的相似对角化！

线性代数的知识告诉我们，对于一般的方阵，并不都是可以相似对角化的，只能相似到一个[若尔当标准型](https://en.wikipedia.org/wiki/Jordan_normal_form)。
但是如果A是对称阵（对于复数则是厄米对称），那么答案就是肯定的！

![特征值方程](/assets/images/Eigenvalue_equation.svg)

假设对于对称矩阵A，两个独立的方向单位向量是$$v_1, v_2$$，那么有

$$
Av_1 = \lambda_1 v_1, \\
Av_2 = \lambda_2 v_2
$$

$$\lambda_1, \lambda_2$$是两个实数，代表这两个独立方向上的伸缩变换比例！
并且这两个方向向量构成一组标准正交基！因此，对于一般的向量x，可以选取这两个向量作为基向量，重新表达

$$
x = x_1 v_1 + x_2 v_2
$$

那么，矩阵A对向量x的变换为

$$
A x = (x_1 \lambda_1) v_1 + (x_2 \lambda_2) v_2
$$

![特征值的含义](/assets/images/Unequal_scaling.svg)

也就是向量x在$$v_1,v_2$$两个独立的正交方向上分别放大了 $$\lambda_1, \lambda_2$$倍！

因此，特征向量可以看做矩阵的多个主方向，在这些方向上，矩阵对应的变换将向量在这些方向上的分量分别伸缩$$\lambda_i$$倍！

利用上述图像，我们很容易得到矩阵的谱范数为最大特征值的绝对值！

$$
\rho(A) = \sup_{||x||_ 2 = 1} ||Ax||_ 2 = \max_i |\lambda_i|
$$

因为谱范数实际上就是矩阵A能把一个单位向量最大放大多少倍！显然在绝对值最大的特征值对应的特征向量方向上具有最大放大倍数！

利用特征值和特征向量的图像，还可以得到幂方法求矩阵特征值的方法。想象有一个普通的向量v，它是如此的普通，以至于不与A的任何特征向量垂直，虽然我们一开始不知道特征向量方向，但是要让v不和它们垂直还是很容易的，随机初始化即可。如果对于这个向量v，不断地应用矩阵A进行变换，那么随着应用次数不断增加，向量$$A^k v$$的方向将无限趋近于绝对值最大特征值对应的特征向量方向！如果该特征值对应的特征向量有多个，那么$$A^k v$$将无限趋近于这个特征子空间！

特征值和特征向量是两个非常普适的概念，在无限维线性空间中也同样重要。

在不同场景下，这些主方向和特征值都是有特定意义的。

例如，在描述曲面上的曲率的时候，会用一个曲率张量（就是一个二阶对称方阵）来描述，该张量的两个特征向量就是两个曲率最大和最小的方向，而两个特征值就是这两个方向的曲率！

平面上的二次曲线都是用一个二次型表示，二次型对应的对称方阵的两个特征向量就是长短轴的方向，而两个特征值就是长短轴的大小（相差一个常数）！

在量子力学中，物理量都是用算符表示，在有限维空间就是矩阵，其对应的特征向量就是本征态，特征值就是物理量的测量值！

## 行列式
初学行列式的时候，对这样一个概念和计算规则非常难理解。
可能很多人都了解过一种解释，**行列式就是列向量张成平行多面体体积（有向体积）**！
这个结论当然是没什么问题的，但是结论不够直观，而且跟矩阵本身的意义缺乏关联。

![平行多面体](/assets/images/Determinant_parallelepiped.svg)

我们可以从线性变换的角度考虑另外一个意义，考虑欧式空间中的一个平行多面体，这个多面体的边长为1，且都是矩阵A的特征向量。
那么这个多面体通过矩阵A变换之后，会变成怎么样呢？
我们知道矩阵A在特征向量方向就是做简单的伸缩变换，伸缩系数就是特征值！
因此，这个多面体经过A变换之后，每个方向上分别放大了$$\lambda_i, i=1,...,n$$倍！
那么它的体积自然就放大了$$\Pi_i \lambda_i$$倍，这正是矩阵A的行列式！

$$
\text{det}(A) = \Pi_i \lambda_i
$$

由此可见，**矩阵A的行列式就是该线性变换的体积放大倍数**！
如果，我们这个平行多面体是由单位基向量构成，经过A变换后就变成了A的列向量对应的平行多面体了，所以 **这个放大倍数自然就是列向量对应的平行多面体有向体积** 了！

综上，我们从线性变换图像的角度，直观解释了行列式的两个意义！他们是一致且直观的！

## 矩阵的逆
有了前面的图像，矩阵的逆就很好解释了，就是逆映射嘛。但是在矩阵这个特殊场景下，还有一些其他更有意思的图像理解。

对于可逆方阵A，$$y = A x$$表示将向量x通过矩阵A变换到y。如果已知y求x，那么就是求逆$$x = A^{-1} y$$。
这个求逆过程可以看做将向量y按照A的列向量展开求对应的系数，也就是用A的列向量作为基向量求对应的坐标！

对于一般矩阵，会有所谓的伪逆。上述线性方程可能无解，可以通过求解最优化问题

$$
\min ||Ax - y||^2
$$

得到最佳x。上述优化问题可以看做用A的列向量来表达y，但是y不在A的列向量所张成的空间中，所以无法精确表达。
所以可以先将y投影到A的列向量所张成的空间中，然后求解

$$
Ax = P_A y
$$

$$P_A$$是到A列空间投影操作，上述方程可以看做向量y投影到A的列空间中，然后以A的列向量为基得到的坐标为x！
